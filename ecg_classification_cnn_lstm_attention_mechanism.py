{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:16:52.669904Z\",\"iopub.execute_input\":\"2023-04-09T14:16:52.670354Z\",\"iopub.status.idle\":\"2023-04-09T14:16:52.678146Z\",\"shell.execute_reply.started\":\"2023-04-09T14:16:52.670317Z\",\"shell.execute_reply\":\"2023-04-09T14:16:52.677028Z\"}}\nimport os\nimport itertools\nimport time\nimport random\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import (CosineAnnealingLR,\n                                      CosineAnnealingWarmRestarts,\n                                      StepLR,\n                                      ExponentialLR)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:16:52.997708Z\",\"iopub.execute_input\":\"2023-04-09T14:16:52.998140Z\",\"iopub.status.idle\":\"2023-04-09T14:16:53.007061Z\",\"shell.execute_reply.started\":\"2023-04-09T14:16:52.998100Z\",\"shell.execute_reply\":\"2023-04-09T14:16:53.005576Z\"}}\nclass Config:\n    csv_path = ''\n    seed = 2021\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    attn_state_path = '../input/mitbih-with-synthetic/attn.pth'\n    lstm_state_path = '../input/mitbih-with-synthetic/lstm.pth'\n    cnn_state_path = '../input/mitbih-with-synthetic/cnn.pth'\n    \n    attn_logs = '../input/mitbih-with-synthetic/attn.csv'\n    lstm_logs = '../input/mitbih-with-synthetic/lstm.csv'\n    cnn_logs = '../input/mitbih-with-synthetic/cnn.csv'\n    \n    train_csv_path = '../input/mitbih-with-synthetic/mitbih_with_syntetic_train.csv'\n    test_csv_path = '../input/mitbih-with-synthetic/mitbih_with_syntetic_test.csv'\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\nconfig = Config()\nseed_everything(config.seed)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:16:53.289514Z\",\"iopub.execute_input\":\"2023-04-09T14:16:53.290112Z\",\"iopub.status.idle\":\"2023-04-09T14:16:57.817183Z\",\"shell.execute_reply.started\":\"2023-04-09T14:16:53.290076Z\",\"shell.execute_reply\":\"2023-04-09T14:16:57.815750Z\"}}\ndf_ptbdb = pd.read_csv('/kaggle/input/heartbeat/ptbdb_abnormal.csv')\ndf_mitbih = pd.read_csv('/kaggle/input/heartbeat/mitbih_train.csv')\ndf_ptbdb\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:16:57.819872Z\",\"iopub.execute_input\":\"2023-04-09T14:16:57.820733Z\",\"iopub.status.idle\":\"2023-04-09T14:16:57.826688Z\",\"shell.execute_reply.started\":\"2023-04-09T14:16:57.820682Z\",\"shell.execute_reply\":\"2023-04-09T14:16:57.825881Z\"}}\nlen(df_ptbdb)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:16:57.827828Z\",\"iopub.execute_input\":\"2023-04-09T14:16:57.828405Z\",\"iopub.status.idle\":\"2023-04-09T14:17:02.771059Z\",\"shell.execute_reply.started\":\"2023-04-09T14:16:57.828365Z\",\"shell.execute_reply\":\"2023-04-09T14:17:02.770196Z\"}}\ndf_mitbih_train = pd.read_csv('/kaggle/input/heartbeat/mitbih_train.csv', header=None)\ndf_mitbih_test = pd.read_csv('/kaggle/input/heartbeat/mitbih_test.csv', header=None)\ndf_mitbih = pd.concat([df_mitbih_train, df_mitbih_test], axis=0)\ndf_mitbih.rename(columns={187: 'class'}, inplace=True)\n\nid_to_label = {\n    0: \"Normal\",\n    1: \"Artial Premature\",\n    2: \"Premature ventricular contraction\",\n    3: \"Fusion of ventricular and normal\",\n    4: \"Fusion of paced and normal\"\n}\ndf_mitbih['label'] = df_mitbih.iloc[:, -1].map(id_to_label)\nprint(df_mitbih.info())\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:17:02.772277Z\",\"iopub.execute_input\":\"2023-04-09T14:17:02.772715Z\",\"iopub.status.idle\":\"2023-04-09T14:17:02.777848Z\",\"shell.execute_reply.started\":\"2023-04-09T14:17:02.772682Z\",\"shell.execute_reply\":\"2023-04-09T14:17:02.776912Z\"}}\nlen(df_mitbih_test)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:17:02.781096Z\",\"iopub.execute_input\":\"2023-04-09T14:17:02.781394Z\",\"iopub.status.idle\":\"2023-04-09T14:17:26.594014Z\",\"shell.execute_reply.started\":\"2023-04-09T14:17:02.781365Z\",\"shell.execute_reply\":\"2023-04-09T14:17:26.593012Z\"}}\ndf_mitbih.to_csv('data.csv', index=False)\nconfig.csv_path = 'data.csv'\n\n# %% [markdown]\n# # Basic EDA\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:17:26.597303Z\",\"iopub.execute_input\":\"2023-04-09T14:17:26.597637Z\",\"iopub.status.idle\":\"2023-04-09T14:17:29.809964Z\",\"shell.execute_reply.started\":\"2023-04-09T14:17:26.597604Z\",\"shell.execute_reply\":\"2023-04-09T14:17:29.808825Z\"}}\ndf_mitbih = pd.read_csv(config.csv_path)\ndf_mitbih['label'].value_counts()\n\n# %% [code] {\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:17:29.811742Z\",\"iopub.execute_input\":\"2023-04-09T14:17:29.812165Z\",\"iopub.status.idle\":\"2023-04-09T14:17:30.430963Z\",\"shell.execute_reply.started\":\"2023-04-09T14:17:29.812029Z\",\"shell.execute_reply\":\"2023-04-09T14:17:30.429761Z\"}}\npercentages = [count / df_mitbih.shape[0] * 100 for count in df_mitbih['label'].value_counts()]\n\nfig, ax = plt.subplots(figsize=(12, 6))\nsns.countplot(\n    x=df_mitbih['label'],\n    ax=ax,\n    palette=\"bright\",\n    order=df_mitbih['label'].value_counts().index\n)\nax.set_xticklabels(ax.get_xticklabels(), rotation=15);\n\nfor percentage, count, p in zip(\n    percentages,\n    df_mitbih['label'].value_counts(sort=True).values,\n    ax.patches):\n    \n    percentage = f'{np.round(percentage, 2)}%'\n    x = p.get_x() + p.get_width() / 2 - 0.4\n    y = p.get_y() + p.get_height()\n    ax.annotate(str(percentage)+\" / \"+str(count), (x, y), fontsize=12, fontweight='bold')\n    \nplt.savefig('data_dist.png', facecolor='w', edgecolor='w', format='png',\n        transparent=False, bbox_inches='tight', pad_inches=0.1)\nplt.savefig('data_dist.svg', facecolor='w', edgecolor='w', format='svg',\n        transparent=False, bbox_inches='tight', pad_inches=0.1)\n\n# %% [markdown]\n# I used the GAN from the notebook you can find [here](https://www.kaggle.com/polomarco/1d-gan-for-ecg-synthesis) or a repository with the code [here](https://github.com/mandrakedrink/ECG-Synthesis-and-Classification) to generate new synthetic data for classes with little data, now the dataset looks like this:\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:17:30.432589Z\",\"iopub.execute_input\":\"2023-04-09T14:17:30.433410Z\",\"iopub.status.idle\":\"2023-04-09T14:17:34.085907Z\",\"shell.execute_reply.started\":\"2023-04-09T14:17:30.433356Z\",\"shell.execute_reply\":\"2023-04-09T14:17:34.084741Z\"}}\nconfig.csv_path = '../input/mitbih-with-synthetic/mitbih_with_syntetic.csv'\ndf_mitbih_new = pd.read_csv(config.csv_path)\n\n# %% [code] {\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:17:34.087775Z\",\"iopub.execute_input\":\"2023-04-09T14:17:34.088111Z\",\"iopub.status.idle\":\"2023-04-09T14:17:35.140666Z\",\"shell.execute_reply.started\":\"2023-04-09T14:17:34.088079Z\",\"shell.execute_reply\":\"2023-04-09T14:17:35.139661Z\"}}\npercentages1 = [count / df_mitbih.shape[0] * 100 for count in df_mitbih['label'].value_counts()]\npercentages2 = [count / df_mitbih_new.shape[0] * 100 for count in df_mitbih_new['label'].value_counts()]\n\nfig, axs = plt.subplots(1,2, figsize=(18, 4))\n\n# origin\nsns.countplot(\n    x=df_mitbih['label'],\n    ax=axs[0],\n    palette=\"bright\",\n    order=df_mitbih['label'].value_counts().index\n)\naxs[0].set_xticklabels(axs[0].get_xticklabels(), rotation=15);\naxs[0].set_title(\"Before\", fontsize=15)\n\nfor percentage, count, p in zip(\n    percentages1,\n    df_mitbih['label'].value_counts(sort=True).values,\n    axs[0].patches):\n    \n    percentage = f'{np.round(percentage, 2)}%'\n    x = p.get_x() + p.get_width() / 2 - 0.4\n    y = p.get_y() + p.get_height()\n    axs[0].annotate(str(percentage)+\" / \"+str(count), (x, y), fontsize=10, fontweight='bold')\n\n# with synthetic\nsns.countplot(\n    x=df_mitbih_new['label'],\n    ax=axs[1],\n    palette=\"bright\",\n    order=df_mitbih_new['label'].value_counts().index\n)\naxs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=15);\naxs[1].set_title(\"After\", fontsize=15)\n\nfor percentage, count, p in zip(\n    percentages2,\n    df_mitbih_new['label'].value_counts(sort=True).values,\n    axs[1].patches):\n    \n    percentage = f'{np.round(percentage, 2)}%'\n    x = p.get_x() + p.get_width() / 2 - 0.4\n    y = p.get_y() + p.get_height()\n    axs[1].annotate(str(percentage)+\" / \"+str(count), (x, y), fontsize=10, fontweight='bold')\n\n#plt.suptitle(\"Balanced Sampling between classes\", fontsize=20, weight=\"bold\", y=1.01)\nplt.savefig('data_dist.png', facecolor='w', edgecolor='w', format='png',\n        transparent=False, bbox_inches='tight', pad_inches=0.1)\nplt.savefig('data_dist.svg', facecolor='w', edgecolor='w', format='svg',\n        transparent=False, bbox_inches='tight', pad_inches=0.1)\n\n# %% [code] {\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:17:35.142027Z\",\"iopub.execute_input\":\"2023-04-09T14:17:35.142323Z\",\"iopub.status.idle\":\"2023-04-09T14:17:37.283196Z\",\"shell.execute_reply.started\":\"2023-04-09T14:17:35.142293Z\",\"shell.execute_reply\":\"2023-04-09T14:17:37.282109Z\"}}\nN = 5\nsamples = [df_mitbih.loc[df_mitbih['class'] == cls].sample(N) for cls in range(N)]\ntitles = [id_to_label[cls] for cls in range(5)]\n\nwith plt.style.context(\"seaborn-white\"):\n    fig, axs = plt.subplots(3, 2, figsize=(20, 7))\n    for i in range(5):\n        ax = axs.flat[i]\n        ax.plot(samples[i].values[:,:-2].transpose())\n        ax.set_title(titles[i])\n        #plt.ylabel(\"Amplitude\")\n\n    plt.tight_layout()\n    plt.suptitle(\"ECG Signals\", fontsize=20, y=1.05, weight=\"bold\")\n    plt.savefig(f\"signals_per_class.svg\",\n                    format=\"svg\",bbox_inches='tight', pad_inches=0.2)\n        \n    plt.savefig(f\"signals_per_class.png\", \n                    format=\"png\",bbox_inches='tight', pad_inches=0.2) \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:17:37.284685Z\",\"iopub.execute_input\":\"2023-04-09T14:17:37.285024Z\",\"iopub.status.idle\":\"2023-04-09T14:19:07.099963Z\",\"shell.execute_reply.started\":\"2023-04-09T14:17:37.284989Z\",\"shell.execute_reply\":\"2023-04-09T14:19:07.098972Z\"}}\n%%time\nsignals = [' '.join(df_mitbih.iloc[i, :-1].apply(str).values) for i in range(df_mitbih.shape[0])]\ny = df_mitbih.iloc[:, -1].values.tolist()\nprint(len(signals), len(y))\n\nprint(f'data has {len(set([sig for line in signals for sig in line.split()]))} out of 16 372 411 unique values.')\n\n# %% [markdown]\n# ## Dataset and DataLoader\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:19:07.101675Z\",\"iopub.execute_input\":\"2023-04-09T14:19:07.102251Z\",\"iopub.status.idle\":\"2023-04-09T14:19:07.116108Z\",\"shell.execute_reply.started\":\"2023-04-09T14:19:07.102214Z\",\"shell.execute_reply\":\"2023-04-09T14:19:07.109149Z\"}}\nclass ECGDataset(Dataset):\n\n    def __init__(self, df):\n        self.df = df\n        self.data_columns = self.df.columns[:-2].tolist()\n\n    def __getitem__(self, idx):\n        signal = self.df.loc[idx, self.data_columns].astype('float32')\n        signal = torch.FloatTensor([signal.values])                 \n        target = torch.LongTensor(np.array(self.df.loc[idx, 'class']))\n        return signal, target\n\n    def __len__(self):\n        return len(self.df)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:19:07.118237Z\",\"iopub.execute_input\":\"2023-04-09T14:19:07.118727Z\",\"iopub.status.idle\":\"2023-04-09T14:19:07.126413Z\",\"shell.execute_reply.started\":\"2023-04-09T14:19:07.118681Z\",\"shell.execute_reply\":\"2023-04-09T14:19:07.125128Z\"}}\ndef get_dataloader(phase: str, batch_size: int = 96) -> DataLoader:\n    '''\n    Dataset and DataLoader.\n    Parameters:\n        pahse: training or validation phase.\n        batch_size: data per iteration.\n    Returns:\n        data generator\n    '''\n    df = pd.read_csv(config.train_csv_path)\n    train_df, val_df = train_test_split(\n        df, test_size=0.15, random_state=config.seed, stratify=df['label']\n    )\n    train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n    df = train_df if phase == 'train' else val_df\n    dataset = ECGDataset(df)\n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=4)\n    return dataloader\n\n# %% [markdown]\n# # Models\n\n# %% [markdown]\n# ![](https://64.media.tumblr.com/e42e20eb2ec1aea3962c6ace63adf499/70877119c7741403-44/s540x810/c8f722eb2ab3d92c98070554db4815ca8c01510b.png)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:19:07.128266Z\",\"iopub.execute_input\":\"2023-04-09T14:19:07.128749Z\",\"iopub.status.idle\":\"2023-04-09T14:19:07.351147Z\",\"shell.execute_reply.started\":\"2023-04-09T14:19:07.128704Z\",\"shell.execute_reply\":\"2023-04-09T14:19:07.349810Z\"}}\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n    \nx = torch.linspace(-10.0, 10.0, 100)\nswish = Swish()\nswish_out = swish(x)\nrelu_out = torch.relu(x)\n\nplt.title('Swish function')\nplt.plot(x.numpy(), swish_out.numpy(), label='Swish')\nplt.plot(x.numpy(), relu_out.numpy(), label='ReLU')\nplt.legend();\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:19:07.352438Z\",\"iopub.execute_input\":\"2023-04-09T14:19:07.352815Z\",\"iopub.status.idle\":\"2023-04-09T14:19:07.367097Z\",\"shell.execute_reply.started\":\"2023-04-09T14:19:07.352782Z\",\"shell.execute_reply\":\"2023-04-09T14:19:07.366097Z\"}}\nclass ConvNormPool(nn.Module):\n    \"\"\"Conv Skip-connection module\"\"\"\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        kernel_size,\n        norm_type='bachnorm'\n    ):\n        super().__init__()\n        \n        self.kernel_size = kernel_size\n        self.conv_1 = nn.Conv1d(\n            in_channels=input_size,\n            out_channels=hidden_size,\n            kernel_size=kernel_size\n        )\n        self.conv_2 = nn.Conv1d(\n            in_channels=hidden_size,\n            out_channels=hidden_size,\n            kernel_size=kernel_size\n        )\n        self.conv_3 = nn.Conv1d(\n            in_channels=hidden_size,\n            out_channels=hidden_size,\n            kernel_size=kernel_size\n        )\n        self.swish_1 = Swish()\n        self.swish_2 = Swish()\n        self.swish_3 = Swish()\n        if norm_type == 'group':\n            self.normalization_1 = nn.GroupNorm(\n                num_groups=8,\n                num_channels=hidden_size\n            )\n            self.normalization_2 = nn.GroupNorm(\n                num_groups=8,\n                num_channels=hidden_size\n            )\n            self.normalization_3 = nn.GroupNorm(\n                num_groups=8,\n                num_channels=hidden_size\n            )\n        else:\n            self.normalization_1 = nn.BatchNorm1d(num_features=hidden_size)\n            self.normalization_2 = nn.BatchNorm1d(num_features=hidden_size)\n            self.normalization_3 = nn.BatchNorm1d(num_features=hidden_size)\n            \n        self.pool = nn.MaxPool1d(kernel_size=2)\n        \n    def forward(self, input):\n        conv1 = self.conv_1(input)\n        x = self.normalization_1(conv1)\n        x = self.swish_1(x)\n        x = F.pad(x, pad=(self.kernel_size - 1, 0))\n        \n        x = self.conv_2(x)\n        x = self.normalization_2(x)\n        x = self.swish_2(x)\n        x = F.pad(x, pad=(self.kernel_size - 1, 0))\n        \n        conv3 = self.conv_3(x)\n        x = self.normalization_3(conv1+conv3)\n        x = self.swish_3(x)\n        x = F.pad(x, pad=(self.kernel_size - 1, 0))   \n        \n        x = self.pool(x)\n        return x\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:19:07.368646Z\",\"iopub.execute_input\":\"2023-04-09T14:19:07.369006Z\",\"iopub.status.idle\":\"2023-04-09T14:19:07.387302Z\",\"shell.execute_reply.started\":\"2023-04-09T14:19:07.368969Z\",\"shell.execute_reply\":\"2023-04-09T14:19:07.386121Z\"}}\nclass CNN(nn.Module):\n    def __init__(\n        self,\n        input_size = 1,\n        hid_size = 256,\n        kernel_size = 5,\n        num_classes = 5,\n    ):\n        \n        super().__init__()\n        \n        self.conv1 = ConvNormPool(\n            input_size=input_size,\n            hidden_size=hid_size,\n            kernel_size=kernel_size,\n        )\n        self.conv2 = ConvNormPool(\n            input_size=hid_size,\n            hidden_size=hid_size//2,\n            kernel_size=kernel_size,\n        )\n        self.conv3 = ConvNormPool(\n            input_size=hid_size//2,\n            hidden_size=hid_size//4,\n            kernel_size=kernel_size,\n        )\n        self.avgpool = nn.AdaptiveAvgPool1d((1))\n        self.fc = nn.Linear(in_features=hid_size//4, out_features=num_classes)\n        \n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.avgpool(x)        \n        # print(x.shape) # num_features * num_channels\n        x = x.view(-1, x.size(1) * x.size(2))\n        x = F.softmax(self.fc(x), dim=1)\n        return x\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:19:07.389260Z\",\"iopub.execute_input\":\"2023-04-09T14:19:07.389604Z\",\"iopub.status.idle\":\"2023-04-09T14:19:07.409046Z\",\"shell.execute_reply.started\":\"2023-04-09T14:19:07.389568Z\",\"shell.execute_reply\":\"2023-04-09T14:19:07.407350Z\"}}\nclass RNN(nn.Module):\n    \"\"\"RNN module(cell type lstm or gru)\"\"\"\n    def __init__(\n        self,\n        input_size,\n        hid_size,\n        num_rnn_layers=1,\n        dropout_p = 0.2,\n        bidirectional = False,\n        rnn_type = 'lstm',\n    ):\n        super().__init__()\n        \n        if rnn_type == 'lstm':\n            self.rnn_layer = nn.LSTM(\n                input_size=input_size,\n                hidden_size=hid_size,\n                num_layers=num_rnn_layers,\n                dropout=dropout_p if num_rnn_layers>1 else 0,\n                bidirectional=bidirectional,\n                batch_first=True,\n            )\n            \n        else:\n            self.rnn_layer = nn.GRU(\n                input_size=input_size,\n                hidden_size=hid_size,\n                num_layers=num_rnn_layers,\n                dropout=dropout_p if num_rnn_layers>1 else 0,\n                bidirectional=bidirectional,\n                batch_first=True,\n            )\n    def forward(self, input):\n        outputs, hidden_states = self.rnn_layer(input)\n        return outputs, hidden_states\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:19:07.410738Z\",\"iopub.execute_input\":\"2023-04-09T14:19:07.411144Z\",\"iopub.status.idle\":\"2023-04-09T14:19:07.426425Z\",\"shell.execute_reply.started\":\"2023-04-09T14:19:07.411099Z\",\"shell.execute_reply\":\"2023-04-09T14:19:07.425170Z\"}}\nclass RNNModel(nn.Module):\n    def __init__(\n        self,\n        input_size,\n        hid_size,\n        rnn_type,\n        bidirectional,\n        n_classes=5,\n        kernel_size=5,\n    ):\n        super().__init__()\n            \n        self.rnn_layer = RNN(\n            input_size=46,#hid_size * 2 if bidirectional else hid_size,\n            hid_size=hid_size,\n            rnn_type=rnn_type,\n            bidirectional=bidirectional\n        )\n        self.conv1 = ConvNormPool(\n            input_size=input_size,\n            hidden_size=hid_size,\n            kernel_size=kernel_size,\n        )\n        self.conv2 = ConvNormPool(\n            input_size=hid_size,\n            hidden_size=hid_size,\n            kernel_size=kernel_size,\n        )\n        self.avgpool = nn.AdaptiveAvgPool1d((1))\n        self.fc = nn.Linear(in_features=hid_size, out_features=n_classes)\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.conv2(x)\n        x, _ = self.rnn_layer(x)\n        x = self.avgpool(x)\n        x = x.view(-1, x.size(1) * x.size(2))\n        x = F.softmax(self.fc(x), dim=1)#.squeeze(1)\n        return x\n\n# %% [markdown]\n# ### \"Attention Mechanism\" Quick Reminder\n# \n# \n# The attention mechanism is best explained with the example of the seq2seq model, so it would be a great idea to read this interactive [article](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html).\n# \n# Models based an architecture such as sequence2sequence, for example, to translate from one language to another, **Attention**, use to clarify the word order, when translating by the decoder, more specificaly to make weights of some words more or less meaningful in the encoder path, for improved translation.\n# \n# \n# ### Excerpt from [article](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html):\n# At each decoder step, attention:\n# + receives attention input: a decoder state ht and all encoder states s1, s2...sk;\n# + computes attention scores;\n# + For each encoder state sk attention computes its \"relevance\" for this decoder state ht. Formally, it applies an attention function which receives one decoder state and one encoder state and returns a scalar value **score(ht, sk)**;\n# + computes attention weights: a probability distribution - softmax applied to attention scores;\n# + computes attention output: the weighted sum of encoder states with attention weights.\n#     \n# The general computation scheme is shown below.\n# <img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/attention/computation_scheme-min.png\" alt=\"drawing\" width=\"60%\" height=\"60%\"/>\n# \n# The most popular ways to compute attention scores are:\n# \n# + dot-product - the simplest method;\n# + bilinear function (aka \"Luong attention\") - used in the paper Effective Approaches to Attention-based Neural Machine Translation;\n# + multi-layer perceptron (aka \"Bahdanau attention\") - the method proposed in the original paper.\n# ![alt](https://lena-voita.github.io/resources/lectures/seq2seq/attention/score_functions-min.png)\n# \n# ---\n# \n# We will use Attention mechanism in ecg classification, \"to clarify\" to give more attention to important features, be it features from recurrent layers or convolutional.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:19:07.428170Z\",\"iopub.execute_input\":\"2023-04-09T14:19:07.428645Z\",\"iopub.status.idle\":\"2023-04-09T14:19:07.446857Z\",\"shell.execute_reply.started\":\"2023-04-09T14:19:07.428578Z\",\"shell.execute_reply\":\"2023-04-09T14:19:07.445668Z\"}}\nclass RNNAttentionModel(nn.Module):\n    def __init__(\n        self,\n        input_size,\n        hid_size,\n        rnn_type,\n        bidirectional,\n        n_classes=5,\n        kernel_size=5,\n    ):\n        super().__init__()\n \n        self.rnn_layer = RNN(\n            input_size=46,\n            hid_size=hid_size,\n            rnn_type=rnn_type,\n            bidirectional=bidirectional\n        )\n        self.conv1 = ConvNormPool(\n            input_size=input_size,\n            hidden_size=hid_size,\n            kernel_size=kernel_size,\n        )\n        self.conv2 = ConvNormPool(\n            input_size=hid_size,\n            hidden_size=hid_size,\n            kernel_size=kernel_size,\n        )\n        self.avgpool = nn.AdaptiveMaxPool1d((1))\n        self.attn = nn.Linear(hid_size, hid_size, bias=False)\n        self.fc = nn.Linear(in_features=hid_size, out_features=n_classes)\n        \n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.conv2(x)\n        x_out, hid_states = self.rnn_layer(x)\n        x = torch.cat([hid_states[0], hid_states[1]], dim=0).transpose(0, 1)\n        x_attn = torch.tanh(self.attn(x))\n        x = x_attn.bmm(x_out)\n        x = x.transpose(2, 1)\n        x = self.avgpool(x)\n        x = x.view(-1, x.size(1) * x.size(2))\n        x = F.softmax(self.fc(x), dim=-1)\n        return x\n\n# %% [markdown]\n# # Training Stage\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:19:07.448322Z\",\"iopub.execute_input\":\"2023-04-09T14:19:07.448751Z\",\"iopub.status.idle\":\"2023-04-09T14:19:07.467965Z\",\"shell.execute_reply.started\":\"2023-04-09T14:19:07.448706Z\",\"shell.execute_reply\":\"2023-04-09T14:19:07.466919Z\"}}\nclass Meter:\n    def __init__(self, n_classes=5):\n        self.metrics = {}\n        self.confusion = torch.zeros((n_classes, n_classes))\n    \n    def update(self, x, y, loss):\n        x = np.argmax(x.detach().cpu().numpy(), axis=1)\n        y = y.detach().cpu().numpy()\n        self.metrics['loss'] += loss\n        self.metrics['accuracy'] += accuracy_score(x,y)\n        self.metrics['f1'] += f1_score(x,y,average='macro')\n        self.metrics['precision'] += precision_score(x, y, average='macro', zero_division=1)\n        self.metrics['recall'] += recall_score(x,y, average='macro', zero_division=1)\n        \n        self._compute_cm(x, y)\n        \n    def _compute_cm(self, x, y):\n        for prob, target in zip(x, y):\n            if prob == target:\n                self.confusion[target][target] += 1\n            else:\n                self.confusion[target][prob] += 1\n    \n    def init_metrics(self):\n        self.metrics['loss'] = 0\n        self.metrics['accuracy'] = 0\n        self.metrics['f1'] = 0\n        self.metrics['precision'] = 0\n        self.metrics['recall'] = 0\n        \n    def get_metrics(self):\n        return self.metrics\n    \n    def get_confusion_matrix(self):\n        return self.confusion\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:19:07.469509Z\",\"iopub.execute_input\":\"2023-04-09T14:19:07.470091Z\",\"iopub.status.idle\":\"2023-04-09T14:19:07.491458Z\",\"shell.execute_reply.started\":\"2023-04-09T14:19:07.470045Z\",\"shell.execute_reply\":\"2023-04-09T14:19:07.490583Z\"}}\nclass Trainer:\n    def __init__(self, net, lr, batch_size, num_epochs):\n        self.net = net.to(config.device)\n        self.num_epochs = num_epochs\n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = AdamW(self.net.parameters(), lr=lr)\n        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=num_epochs, eta_min=5e-6)\n        self.best_loss = float('inf')\n        self.phases = ['train', 'val']\n        self.dataloaders = {\n            phase: get_dataloader(phase, batch_size) for phase in self.phases\n        }\n        self.train_df_logs = pd.DataFrame()\n        self.val_df_logs = pd.DataFrame()\n    \n    def _train_epoch(self, phase):\n        print(f\"{phase} mode | time: {time.strftime('%H:%M:%S')}\")\n        \n        self.net.train() if phase == 'train' else self.net.eval()\n        meter = Meter()\n        meter.init_metrics()\n        \n        for i, (data, target) in enumerate(self.dataloaders[phase]):\n            data = data.to(config.device)\n            target = target.to(config.device)\n            \n            output = self.net(data)\n            loss = self.criterion(output, target)\n                        \n            if phase == 'train':\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n            \n            meter.update(output, target, loss.item())\n        \n        metrics = meter.get_metrics()\n        metrics = {k:v / i for k, v in metrics.items()}\n        df_logs = pd.DataFrame([metrics])\n        confusion_matrix = meter.get_confusion_matrix()\n        \n        if phase == 'train':\n            self.train_df_logs = pd.concat([self.train_df_logs, df_logs], axis=0)\n        else:\n            self.val_df_logs = pd.concat([self.val_df_logs, df_logs], axis=0)\n        \n        # show logs\n        print('{}: {}, {}: {}, {}: {}, {}: {}, {}: {}'\n              .format(*(x for kv in metrics.items() for x in kv))\n             )\n        fig, ax = plt.subplots(figsize=(5, 5))\n        cm_ = ax.imshow(confusion_matrix, cmap='hot')\n        ax.set_title('Confusion matrix', fontsize=15)\n        ax.set_xlabel('Actual', fontsize=13)\n        ax.set_ylabel('Predicted', fontsize=13)\n        plt.colorbar(cm_)\n        plt.show()\n        \n        return loss\n    \n    def run(self):\n        for epoch in range(self.num_epochs):\n            self._train_epoch(phase='train')\n            with torch.no_grad():\n                val_loss = self._train_epoch(phase='val')\n                self.scheduler.step()\n            \n            if val_loss < self.best_loss:\n                self.best_loss = val_loss\n                print('\\nNew checkpoint\\n')\n                self.best_loss = val_loss\n                torch.save(self.net.state_dict(), f\"best_model_epoc{epoch}.pth\")\n            #clear_output()\n        \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:19:07.492922Z\",\"iopub.execute_input\":\"2023-04-09T14:19:07.493244Z\",\"iopub.status.idle\":\"2023-04-09T14:19:07.513416Z\",\"shell.execute_reply.started\":\"2023-04-09T14:19:07.493212Z\",\"shell.execute_reply\":\"2023-04-09T14:19:07.512600Z\"}}\n#model = RNNAttentionModel(1, 64, 'lstm', False)\nmodel = RNNModel(1, 64, 'lstm', True)\n#model = CNN(num_classes=5, hid_size=128)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T14:19:07.515981Z\",\"iopub.execute_input\":\"2023-04-09T14:19:07.516618Z\",\"iopub.status.idle\":\"2023-04-09T15:11:34.266500Z\",\"shell.execute_reply.started\":\"2023-04-09T14:19:07.516569Z\",\"shell.execute_reply\":\"2023-04-09T15:11:34.265299Z\"}}\ntrainer = Trainer(net=model, lr=1e-3, batch_size=96, num_epochs=10)#100)\ntrainer.run()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:11:34.268640Z\",\"iopub.execute_input\":\"2023-04-09T15:11:34.268982Z\",\"iopub.status.idle\":\"2023-04-09T15:11:34.282767Z\",\"shell.execute_reply.started\":\"2023-04-09T15:11:34.268949Z\",\"shell.execute_reply\":\"2023-04-09T15:11:34.281729Z\"}}\ntrain_logs = trainer.train_df_logs\ntrain_logs.columns = [\"train_\"+ colname for colname in train_logs.columns]\nval_logs = trainer.val_df_logs\nval_logs.columns = [\"val_\"+ colname for colname in val_logs.columns]\n\nlogs = pd.concat([train_logs,val_logs], axis=1)\nlogs.reset_index(drop=True, inplace=True)\nlogs = logs.loc[:, [\n    'train_loss', 'val_loss', \n    'train_accuracy', 'val_accuracy', \n    'train_f1', 'val_f1',\n    'train_precision', 'val_precision',\n    'train_recall', 'val_recall']\n                                 ]\nlogs.head()\nlogs.to_csv('cnn.csv', index=False)\n\n# %% [markdown]\n# # Experiments and Results\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:11:34.284713Z\",\"iopub.execute_input\":\"2023-04-09T15:11:34.285064Z\",\"iopub.status.idle\":\"2023-04-09T15:11:34.329872Z\",\"shell.execute_reply.started\":\"2023-04-09T15:11:34.285031Z\",\"shell.execute_reply\":\"2023-04-09T15:11:34.328834Z\"}}\ncnn_model = CNN(num_classes=5, hid_size=128).to(config.device)\ncnn_model.load_state_dict(\n    torch.load(config.cnn_state_path,\n               map_location=config.device)\n);\ncnn_model.eval();\nlogs = pd.read_csv(config.cnn_logs)\n\n# %% [code] {\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:11:34.332081Z\",\"iopub.execute_input\":\"2023-04-09T15:11:34.332594Z\",\"iopub.status.idle\":\"2023-04-09T15:11:35.496183Z\",\"shell.execute_reply.started\":\"2023-04-09T15:11:34.332519Z\",\"shell.execute_reply\":\"2023-04-09T15:11:35.494933Z\"}}\ncolors = ['#C042FF', '#03C576FF', '#FF355A', '#03C5BF', '#96C503', '#C5035B']\npalettes = [sns.color_palette(colors, 2),\n            sns.color_palette(colors, 4), \n            sns.color_palette(colors[:2]+colors[-2:] + colors[2:-2], 6)]\n            \nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\nsns.lineplot(data=logs.iloc[:, :2], palette=palettes[0], markers=True, ax=ax[0], linewidth=2.5,)\nax[0].set_title(\"Loss Function during Model Training\", fontsize=14)\nax[0].set_xlabel(\"Epoch\", fontsize=14)\n\nsns.lineplot(data=logs.iloc[:, 2:6], palette=palettes[1], markers=True, ax=ax[1], linewidth=2.5, legend=\"full\")\nax[1].set_title(\"Metrics during Model Training\", fontsize=15)\nax[1].set_xlabel(\"Epoch\", fontsize=14)\n\nplt.suptitle('CNN Model', fontsize=18)\n\nplt.tight_layout()\nfig.savefig(\"cnn.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\nfig.savefig(\"cnn.svg\", format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:11:35.497944Z\",\"iopub.execute_input\":\"2023-04-09T15:11:35.498262Z\",\"iopub.status.idle\":\"2023-04-09T15:11:35.523992Z\",\"shell.execute_reply.started\":\"2023-04-09T15:11:35.498231Z\",\"shell.execute_reply\":\"2023-04-09T15:11:35.522903Z\"}}\nlstm_model = RNNModel(1, 64, 'lstm', True).to(config.device)\nlstm_model.load_state_dict(\n    torch.load(config.lstm_state_path,\n               map_location=config.device)\n);\nlstm_model.eval();\nlogs = pd.read_csv(config.lstm_logs)\n\n# %% [code] {\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:11:35.525560Z\",\"iopub.execute_input\":\"2023-04-09T15:11:35.525876Z\",\"iopub.status.idle\":\"2023-04-09T15:11:36.672287Z\",\"shell.execute_reply.started\":\"2023-04-09T15:11:35.525846Z\",\"shell.execute_reply\":\"2023-04-09T15:11:36.671079Z\"}}\ncolors = ['#C042FF', '#03C576FF', '#FF355A', '#03C5BF', '#96C503', '#C5035B']\npalettes = [sns.color_palette(colors, 2),\n            sns.color_palette(colors, 4), \n            sns.color_palette(colors[:2]+colors[-2:] + colors[2:-2], 6)]\n            \nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\nsns.lineplot(data=logs.iloc[:, :2], palette=palettes[0], markers=True, ax=ax[0], linewidth=2.5,)\nax[0].set_title(\"Loss Function during Model Training\", fontsize=14)\nax[0].set_xlabel(\"Epoch\", fontsize=14)\n\nsns.lineplot(data=logs.iloc[:, 2:6], palette=palettes[1], markers=True, ax=ax[1], linewidth=2.5, legend=\"full\")\nax[1].set_title(\"Metrics during Model Training\", fontsize=15)\nax[1].set_xlabel(\"Epoch\", fontsize=14)\n\nplt.suptitle('CNN+LSTM Model', fontsize=18)\n\nplt.tight_layout()\nfig.savefig(\"lstm.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\nfig.savefig(\"lstm.svg\", format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:11:36.673899Z\",\"iopub.execute_input\":\"2023-04-09T15:11:36.674231Z\",\"iopub.status.idle\":\"2023-04-09T15:11:36.698966Z\",\"shell.execute_reply.started\":\"2023-04-09T15:11:36.674197Z\",\"shell.execute_reply\":\"2023-04-09T15:11:36.697976Z\"}}\nattn_model = RNNAttentionModel(1, 64, 'lstm', False).to(config.device)\nattn_model.load_state_dict(\n    torch.load(config.attn_state_path,\n               map_location=config.device)\n);\nattn_model.eval();\nlogs = pd.read_csv(config.attn_logs)\n\n# %% [code] {\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:11:36.700479Z\",\"iopub.execute_input\":\"2023-04-09T15:11:36.700839Z\",\"iopub.status.idle\":\"2023-04-09T15:11:37.871938Z\",\"shell.execute_reply.started\":\"2023-04-09T15:11:36.700805Z\",\"shell.execute_reply\":\"2023-04-09T15:11:37.870747Z\"}}\ncolors = ['#C042FF', '#03C576FF', '#FF355A', '#03C5BF', '#96C503', '#C5035B']\npalettes = [sns.color_palette(colors, 2),\n            sns.color_palette(colors, 4), \n            sns.color_palette(colors[:2]+colors[-2:] + colors[2:-2], 6)]\n            \nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\nsns.lineplot(data=logs.iloc[:, :2], palette=palettes[0], markers=True, ax=ax[0], linewidth=2.5,)\nax[0].set_title(\"Loss Function during Model Training\", fontsize=14)\nax[0].set_xlabel(\"Epoch\", fontsize=14)\n\nsns.lineplot(data=logs.iloc[:, 2:6], palette=palettes[1], markers=True, ax=ax[1], linewidth=2.5, legend=\"full\")\nax[1].set_title(\"Metrics during Model Training\", fontsize=15)\nax[1].set_xlabel(\"Epoch\", fontsize=14)\n\nplt.suptitle('CNN+LSTM+Attention Model', fontsize=18)\n\nplt.tight_layout()\nfig.savefig(\"attn.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\nfig.savefig(\"attn.svg\", format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n\n# %% [markdown]\n# ## Experiments and Results for Test Stage\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:11:37.873220Z\",\"iopub.execute_input\":\"2023-04-09T15:11:37.873526Z\",\"iopub.status.idle\":\"2023-04-09T15:11:38.510942Z\",\"shell.execute_reply.started\":\"2023-04-09T15:11:37.873497Z\",\"shell.execute_reply\":\"2023-04-09T15:11:38.509681Z\"}}\ntest_df = pd.read_csv(config.test_csv_path)\nprint(test_df.shape)\ntest_dataset = ECGDataset(test_df)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=96, num_workers=0, shuffle=False)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:11:38.517261Z\",\"iopub.execute_input\":\"2023-04-09T15:11:38.517857Z\",\"iopub.status.idle\":\"2023-04-09T15:11:38.525750Z\",\"shell.execute_reply.started\":\"2023-04-09T15:11:38.517805Z\",\"shell.execute_reply\":\"2023-04-09T15:11:38.524618Z\"}}\ndef make_test_stage(dataloader, model, probs=False):\n    cls_predictions = []\n    cls_ground_truths = []\n\n    for i, (data, cls_target) in enumerate(dataloader):\n        with torch.no_grad():\n\n            data = data.to(config.device)\n            cls_target = cls_target.cpu()\n            cls_prediction = model(data)\n            \n            if not probs:\n                cls_prediction = torch.argmax(cls_prediction, dim=1)\n    \n            cls_predictions.append(cls_prediction.detach().cpu())\n            cls_ground_truths.append(cls_target)\n\n    predictions_cls = torch.cat(cls_predictions).numpy()\n    ground_truths_cls = torch.cat(cls_ground_truths).numpy()\n    return predictions_cls, ground_truths_cls\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:11:38.527734Z\",\"iopub.execute_input\":\"2023-04-09T15:11:38.528196Z\",\"iopub.status.idle\":\"2023-04-09T15:11:38.544077Z\",\"shell.execute_reply.started\":\"2023-04-09T15:11:38.528148Z\",\"shell.execute_reply\":\"2023-04-09T15:11:38.542841Z\"}}\nmodels = [cnn_model, lstm_model, attn_model]\n\n\n# %% [markdown]\n# ### cnn model report\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:11:38.546597Z\",\"iopub.execute_input\":\"2023-04-09T15:11:38.547271Z\",\"iopub.status.idle\":\"2023-04-09T15:12:17.929800Z\",\"shell.execute_reply.started\":\"2023-04-09T15:11:38.547216Z\",\"shell.execute_reply\":\"2023-04-09T15:12:17.925384Z\"}}\ny_pred, y_true = make_test_stage(test_dataloader, models[0])\ny_pred.shape, y_true.shape\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:12:17.931058Z\",\"iopub.execute_input\":\"2023-04-09T15:12:17.931509Z\",\"iopub.status.idle\":\"2023-04-09T15:12:17.964528Z\",\"shell.execute_reply.started\":\"2023-04-09T15:12:17.931475Z\",\"shell.execute_reply\":\"2023-04-09T15:12:17.963662Z\"}}\nreport = pd.DataFrame(\n    classification_report(\n        y_pred,\n        y_true,\n        output_dict=True\n    )\n).transpose()\n\n# %% [code] {\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:12:17.965841Z\",\"iopub.execute_input\":\"2023-04-09T15:12:17.966311Z\",\"iopub.status.idle\":\"2023-04-09T15:12:18.899925Z\",\"shell.execute_reply.started\":\"2023-04-09T15:12:17.966277Z\",\"shell.execute_reply\":\"2023-04-09T15:12:18.898667Z\"}}\ncolors = ['#00FA9A', '#D2B48C', '#FF69B4']#random.choices(list(mcolors.CSS4_COLORS.values()), k = 3)\nreport_plot = report.apply(lambda x: x*100)\nax = report_plot[[\"precision\", \"recall\", \"f1-score\"]].plot(kind='bar',\n                                                      figsize=(13, 4), legend=True, fontsize=15, color=colors)\n\nax.set_xlabel(\"Estimators\", fontsize=15)\nax.set_xticklabels(\n    list(id_to_label.values())+[\"accuracy avg\", \"marco avg\", \"weighted avg\"],\n    rotation=15, fontsize=11)\nax.set_ylabel(\"Percentage\", fontsize=15)\nplt.title(\"CNN Model Classification Report\", fontsize=20)\n\nfor percentage, p in zip(\n    report[['precision', 'recall', 'f1-score']].values,\n    ax.patches):\n    \n    percentage = \" \".join([str(round(i*100, 2))+\"%\" for i in percentage])\n    x = p.get_x() + p.get_width() - 0.4\n    y = p.get_y() + p.get_height() / 4\n    ax.annotate(percentage, (x, y), fontsize=8, rotation=15, fontweight='bold')\nfig.savefig(\"cnn_report.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\nfig.savefig(\"cnn_report.svg\", format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\nplt.show()\n\n# %% [markdown]\n# ### cnn+lstm model report\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:12:18.901637Z\",\"iopub.execute_input\":\"2023-04-09T15:12:18.901996Z\",\"iopub.status.idle\":\"2023-04-09T15:12:53.702612Z\",\"shell.execute_reply.started\":\"2023-04-09T15:12:18.901929Z\",\"shell.execute_reply\":\"2023-04-09T15:12:53.701323Z\"}}\ny_pred, y_true = make_test_stage(test_dataloader, models[1])\ny_pred.shape, y_true.shape\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:12:53.704609Z\",\"iopub.execute_input\":\"2023-04-09T15:12:53.705469Z\",\"iopub.status.idle\":\"2023-04-09T15:12:53.748313Z\",\"shell.execute_reply.started\":\"2023-04-09T15:12:53.705409Z\",\"shell.execute_reply\":\"2023-04-09T15:12:53.747191Z\"}}\nreport = pd.DataFrame(\n    classification_report(\n        y_pred,\n        y_true,\n        output_dict=True\n    )\n).transpose()\n\n# %% [code] {\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:12:53.750230Z\",\"iopub.execute_input\":\"2023-04-09T15:12:53.750688Z\",\"iopub.status.idle\":\"2023-04-09T15:12:54.768031Z\",\"shell.execute_reply.started\":\"2023-04-09T15:12:53.750642Z\",\"shell.execute_reply\":\"2023-04-09T15:12:54.766403Z\"}}\ncolors = ['#00FA9A', '#D2B48C', '#FF69B4']#random.choices(list(mcolors.CSS4_COLORS.values()), k = 3)\nreport_plot = report.apply(lambda x: x*100)\nax = report_plot[[\"precision\", \"recall\", \"f1-score\"]].plot(kind='bar',\n                                                      figsize=(13, 4), legend=True, fontsize=15, color=colors)\n\nax.set_xlabel(\"Estimators\", fontsize=15)\nax.set_xticklabels(\n    list(id_to_label.values())+[\"accuracy avg\", \"marco avg\", \"weighted avg\"],\n    rotation=15, fontsize=11)\nax.set_ylabel(\"Percentage\", fontsize=15)\nplt.title(\"CNN+LSTM Model Classification Report\", fontsize=20)\n\nfor percentage, p in zip(\n    report[['precision', 'recall', 'f1-score']].values,\n    ax.patches):\n    \n    percentage = \" \".join([str(round(i*100, 2))+\"%\" for i in percentage])\n    x = p.get_x() + p.get_width() - 0.4\n    y = p.get_y() + p.get_height() / 4\n    ax.annotate(percentage, (x, y), fontsize=8, rotation=15, fontweight='bold')\nfig.savefig(\"lstm_report.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\nfig.savefig(\"lstm_report.svg\", format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\nplt.show()\n\n# %% [markdown]\n# ### cnn+lstm+attention model report\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:12:54.769531Z\",\"iopub.execute_input\":\"2023-04-09T15:12:54.769899Z\",\"iopub.status.idle\":\"2023-04-09T15:13:26.337035Z\",\"shell.execute_reply.started\":\"2023-04-09T15:12:54.769863Z\",\"shell.execute_reply\":\"2023-04-09T15:13:26.336043Z\"}}\ny_pred, y_true = make_test_stage(test_dataloader, models[2])\ny_pred.shape, y_true.shape\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:13:26.338623Z\",\"iopub.execute_input\":\"2023-04-09T15:13:26.338945Z\",\"iopub.status.idle\":\"2023-04-09T15:13:26.371878Z\",\"shell.execute_reply.started\":\"2023-04-09T15:13:26.338913Z\",\"shell.execute_reply\":\"2023-04-09T15:13:26.370868Z\"}}\nreport = pd.DataFrame(\n    classification_report(\n        y_pred,\n        y_true,\n        output_dict=True\n    )\n).transpose()\n\n# %% [code] {\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:13:26.373304Z\",\"iopub.execute_input\":\"2023-04-09T15:13:26.373636Z\",\"iopub.status.idle\":\"2023-04-09T15:13:27.285357Z\",\"shell.execute_reply.started\":\"2023-04-09T15:13:26.373604Z\",\"shell.execute_reply\":\"2023-04-09T15:13:27.284198Z\"}}\ncolors = ['#00FA9A', '#D2B48C', '#FF69B4']#random.choices(list(mcolors.CSS4_COLORS.values()), k = 3)\nreport_plot = report.apply(lambda x: x*100)\nax = report_plot[[\"precision\", \"recall\", \"f1-score\"]].plot(kind='bar',\n                                                      figsize=(13, 4), legend=True, fontsize=15, color=colors)\n\nax.set_xlabel(\"Estimators\", fontsize=15)\nax.set_xticklabels(\n    list(id_to_label.values())+[\"accuracy avg\", \"marco avg\", \"weighted avg\"],\n    rotation=15, fontsize=11)\nax.set_ylabel(\"Percentage\", fontsize=15)\nplt.title(\"CNN+LSTM+Attention Model Classification Report\", fontsize=20)\n\nfor percentage, p in zip(\n    report[['precision', 'recall', 'f1-score']].values,\n    ax.patches):\n    \n    percentage = \" \".join([str(round(i*100, 2))+\"%\" for i in percentage])\n    x = p.get_x() + p.get_width() - 0.4\n    y = p.get_y() + p.get_height() / 4\n    ax.annotate(percentage, (x, y), fontsize=8, rotation=15, fontweight='bold')\nfig.savefig(\"attn_report.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\nfig.savefig(\"attn_report.svg\", format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\nplt.show()\n\n# %% [markdown]\n# ### Ensemble of all models\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:13:27.287284Z\",\"iopub.execute_input\":\"2023-04-09T15:13:27.287983Z\",\"iopub.status.idle\":\"2023-04-09T15:15:12.257269Z\",\"shell.execute_reply.started\":\"2023-04-09T15:13:27.287933Z\",\"shell.execute_reply\":\"2023-04-09T15:15:12.256176Z\"}}\ny_pred = np.zeros((y_pred.shape[0], 5), dtype=np.float32)\nfor i, model in enumerate(models, 1):\n    y_pred_, y_true = make_test_stage(test_dataloader, model, True)\n    y_pred += y_pred_\ny_pred /= i\ny_pred = np.argmax(y_pred, axis=1)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:15:12.261919Z\",\"iopub.execute_input\":\"2023-04-09T15:15:12.262436Z\",\"iopub.status.idle\":\"2023-04-09T15:15:13.176240Z\",\"shell.execute_reply.started\":\"2023-04-09T15:15:12.262386Z\",\"shell.execute_reply\":\"2023-04-09T15:15:13.174780Z\"}}\nclf_report = classification_report(y_pred, \n                                   y_true,\n                                   labels=[0,1,2,3,4],\n                                   target_names=list(id_to_label.values()),#['N', 'S', 'V', 'F', 'Q'],\n                                   output_dict=True)\n\n\nplt.figure(figsize=(10, 8))\nax = sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=15)\nax.set_yticklabels(ax.get_yticklabels(),fontsize=12, rotation=0)\nplt.title(\"Ensemble Classification Report\", fontsize=20)\nplt.savefig(f\"ensemble result.svg\",format=\"svg\",bbox_inches='tight', pad_inches=0.2)\nplt.savefig(f\"ensemble result.png\", format=\"png\",bbox_inches='tight', pad_inches=0.2)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-04-09T15:15:13.177825Z\",\"iopub.execute_input\":\"2023-04-09T15:15:13.178151Z\",\"iopub.status.idle\":\"2023-04-09T15:15:13.186978Z\",\"shell.execute_reply.started\":\"2023-04-09T15:15:13.178120Z\",\"shell.execute_reply\":\"2023-04-09T15:15:13.185672Z\"}}\nclf_report\n\n# %% [code]\n\n\n# %% [code]\n\n\n# %% [code]\n","metadata":{"_uuid":"fe52c94e-28f6-4ecb-92d1-180f6315e224","_cell_guid":"567e8e28-e19c-409e-9aab-749ec09b329e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}